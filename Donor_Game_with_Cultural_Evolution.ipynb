{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "collapsed": true,
        "id": "vefO26lrLEgU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf8b50c0-f611-4bd7-cd28-65311949cd7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.109.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy) (2.0.2)\n",
            "Requirement already satisfied: anthropic in /usr/local/lib/python3.12/dist-packages (0.72.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.9.0)\n",
            "Requirement already satisfied: docstring-parser<1,>=0.15 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.17.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (0.11.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from anthropic) (2.11.10)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from anthropic) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.25.0->anthropic) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->anthropic) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.4.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement threading (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for threading\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.43.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n",
        "!pip install scipy\n",
        "!pip install anthropic\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install threading\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "metadata": {
        "id": "hcpVn3diLGD_"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "# import openai\n",
        "import random\n",
        "# from openai import OpenAI\n",
        "from dataclasses import dataclass, field, asdict\n",
        "import os\n",
        "from os import name\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.stats import truncnorm\n",
        "import matplotlib.pyplot as plt\n",
        "import anthropic\n",
        "\n",
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import GenerationConfig\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "import datetime\n",
        "import json\n",
        "from dataclasses import asdict\n",
        "\n",
        "import re\n",
        "import time\n",
        "from anthropic import InternalServerError\n",
        "\n",
        "import threading\n",
        "from threading import Lock\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from queue import Queue\n",
        "\n",
        "from glob import glob\n",
        "from typing import List, Tuple\n",
        "\n",
        "import wandb\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
        "\n",
        "from google.colab import userdata, drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "metadata": {
        "id": "el1HK8YvlClH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72595c4-6bf2-42c1-edab-d594981613cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 187,
      "metadata": {
        "id": "XVNgPJ--OYj6"
      },
      "outputs": [],
      "source": [
        "# Create a global lock\n",
        "print_lock = threading.Lock()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "ApUxYr1oLG-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6680829a-7b02-435b-c467-ee38c3a17d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ],
      "source": [
        "# Set API keys\n",
        "# OPENAI_API_KEY=userdata.get('OPENAI_API_KEY')\n",
        "# ANTHROPIC_API_KEY=userdata.get('ANTHROPIC_API_KEY')\n",
        "GOOGLE_API_KEY=userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# openAI = OpenAI(api_key=OPENAI_API_KEY)\n",
        "# anthropic = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "google = genai.configure(api_key=GOOGLE_API_KEY)\n",
        "wandb.login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "OkT8nHa2P3T9"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Agent:\n",
        "    name: str\n",
        "    resources: int\n",
        "    reputation: float\n",
        "    total_donated: int = 0\n",
        "    potential_donated: int = 0\n",
        "    history: list = field(default_factory=list)\n",
        "    strategy: str = \"\"\n",
        "    strategy_justification: str = \"\"\n",
        "    total_final_score: int = 0\n",
        "    average_reputation: float = 0\n",
        "    traces: list[list[str]] = field(default_factory=lambda: [[]])\n",
        "    old_traces: list[list[str]] = field(default_factory=lambda: [[]])\n",
        "    punishment: int = 0\n",
        "\n",
        "    def donate(self, amount):\n",
        "        \"\"\" Handle the donation process for the agent \"\"\"\n",
        "        if 0 <= amount <= self.resources:\n",
        "            self.resources -= amount\n",
        "            self.total_donated += amount\n",
        "        self.potential_donated += self.resources + amount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "0WvPP46kKnBV"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SimulationData:\n",
        "    hyperparameters: dict\n",
        "    agents_data: list = field(default_factory=list)\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {\n",
        "            'hyperparameters': self.hyperparameters,\n",
        "            'agents_data': self.agents_data\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class AgentRoundData:\n",
        "    agent_name: str\n",
        "    round_number: int\n",
        "    game_number: int\n",
        "    paired_with: str\n",
        "    current_generation: int\n",
        "    resources: int\n",
        "    donated: float\n",
        "    received: float\n",
        "    strategy: str\n",
        "    strategy_justification: str\n",
        "    reputation: float\n",
        "    is_donor: bool\n",
        "    traces: list[list[str]]\n",
        "    history: list\n",
        "    justification: str = \"\"\n",
        "    punished: bool = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "id": "anWz-4i_bFYY"
      },
      "outputs": [],
      "source": [
        "def parse_strategy_output(output):\n",
        "    parts = output.split(\"My strategy will be\", 1)\n",
        "    if len(parts) == 2:\n",
        "        strategy_justification = parts[0].strip()\n",
        "        strategy = \"My strategy will be\" + parts[1].strip()\n",
        "    else:\n",
        "        strategy_justification = \"\"\n",
        "        strategy = output.strip()\n",
        "    return strategy_justification, strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "Fyv3tsPE3vBE"
      },
      "outputs": [],
      "source": [
        "def generate_strategy(agent_name: str, generationNumber: int, inherited_strategies: str) -> str:\n",
        "    if punishment_mechanism == \"costly_punishment\":\n",
        "      punishment_text = f\"You may also choose to punish the recipient by spending x units to take away {punishmentLoss}x of their resources. Bear in mind that others may punish you too.\"\n",
        "    elif punishment_mechanism == \"partner_choice\":\n",
        "      punishment_text = \"You may also choose to punish the recipient by refraining from any interaction. Bear in mind that others may punish you too.\"\n",
        "    else:\n",
        "      punishment_text = \"\"\n",
        "    if generationNumber == 1:\n",
        "      prompt = (\n",
        "        f\"Your name is {agent_name}. \"\n",
        "        \"Based on the description of the game, create a strategy that you will follow in the game. \"\n",
        "        \"As a donor, you will receive limited information about the recipient and their recent interactions:\"\n",
        "        \"You will see what the recipient (let's call them A) did in the immediately preceding round only.\"\n",
        "        \"You will also see what A's partner from that round (let's call them B) did in their preceding round.\"\n",
        "        \"This chain continues for at most three rounds back.\"\n",
        "        \"Important: You do NOT have access to multiple rounds of information about A directly. You only know what A did in the single most recent round.\"\n",
        "        \"Example (assuming it's now round 4):\"\n",
        "        \"You are paired with A.\"\n",
        "        \"You know: In round 3, A donated X% to B.\"\n",
        "        \"You also know: In round 2, B donated Y% to C.\"\n",
        "        \"And finally: In round 1, C donated Z% to D.\"\n",
        "        \"Remember: This is all the information you have. You don't know what A did in rounds 1 or 2, only in round 3.\"\n",
        "        f\"{punishment_text}\"\n",
        "        \"In the first round, there will be no information about the recipient's previous behavior - think carefully about how much to donate in that case.\"\n",
        "        \"Before formulating your strategy, briefly think step by step about what would be a successful strategy in this game.\"\n",
        "        \"Then describe your strategy briefly without explanation in one sentence that starts: My strategy will be.\"\n",
        "    )\n",
        "    else:\n",
        "      prompt = (\n",
        "        f\"Your name is {agent_name}. \"\n",
        "        \"How would you approach the game? Here is the advice of the best-performing 50% of the previous generation, along with their final scores.\"\n",
        "        \"Modify this advice to create your own strategy.\"\n",
        "        f\"{inherited_strategies}\"\n",
        "       \"As a donor, you will receive limited information about the recipient and their recent interactions:\"\n",
        "        \"You will see what the recipient (let's call them A) did in the immediately preceding round only.\"\n",
        "        \"You will also see what A's partner from that round (let's call them B) did in their preceding round.\"\n",
        "        \"This chain continues for at most three rounds back.\"\n",
        "        \"Important: You do NOT have access to multiple rounds of information about A directly. You only know what A did in the single most recent round.\"\n",
        "        \"Example (assuming it's now round 4):\"\n",
        "        \"You are paired with A.\"\n",
        "        \"You know: In round 3, A donated X% to B.\"\n",
        "        \"You also know: In round 2, B donated Y% to C.\"\n",
        "        \"And finally: In round 1, C donated Z% to D.\"\n",
        "        \"Remember: This is all the information you have. You don't know what A did in rounds 1 or 2, only in round 3.\"\n",
        "        f\"{punishment_text}\"\n",
        "        \"In the first round, there will be no information about the recipient's previous behavior - think carefully about how much to donate in that case.\"\n",
        "        \"Before formulating your strategy, briefly think step by step about what would be a successful strategy in this game. In particular, think about how you can improve on the surviving agents' strategies.\"\n",
        "        \"Then describe your strategy briefly without explanation in one sentence that starts: My strategy will be.\"\n",
        "    )\n",
        "    strategy_output = promptLLM(prompt)\n",
        "    strategy_justification, strategy = parse_strategy_output(strategy_output)\n",
        "\n",
        "    print(f\"{agent_name}: \\n Justification: {strategy_justification} \\n Strategy: {strategy} \")\n",
        "    return strategy_justification, strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "5r1a4nbVTKtK"
      },
      "outputs": [],
      "source": [
        "def initializeAgents(numAgents: int, initialEndowment: int, generationNumber: int, inherited_strategies: list) -> list:\n",
        "    agents = []\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        futures = []\n",
        "        for i in range(numAgents):\n",
        "            name = f\"{generationNumber}_{i+1}\"\n",
        "            futures.append(executor.submit(generate_strategy, str(name), generationNumber, inherited_strategies))\n",
        "\n",
        "        # Collect results and create agents\n",
        "        for i, future in enumerate(futures):\n",
        "            strategy_justification, new_strategy = future.result()\n",
        "            name = f\"{generationNumber}_{i+1}\"\n",
        "            agents.append(Agent(name=name, reputation=False, resources=initialEndowment, strategy=new_strategy, strategy_justification=strategy_justification))\n",
        "\n",
        "    random.shuffle(agents)\n",
        "    return agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "QWrEmoGiVxot"
      },
      "outputs": [],
      "source": [
        "def bipartiteRoundRobin(agents):\n",
        "  num_agents = len(agents)\n",
        "  assert num_agents % 2 == 0, \"Number of agents must be even.\"\n",
        "  group_A = agents[:num_agents // 2]\n",
        "  group_B = agents[num_agents // 2:]\n",
        "  rounds = []\n",
        "  toggle_roles = False\n",
        "  # We rotate group B around group A, group A is static in this example\n",
        "  for i in range(len(group_A)):\n",
        "    # Rotate group B\n",
        "    rotated_group_B = group_B[-i:] + group_B[:-i]\n",
        "    if toggle_roles:\n",
        "      round_pairings = list(zip(rotated_group_B, group_A))\n",
        "    else:\n",
        "      round_pairings = list(zip(group_A, rotated_group_B))\n",
        "    rounds.append(round_pairings)\n",
        "    toggle_roles = not toggle_roles\n",
        "  return rounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "id": "yOePSA-umw6F"
      },
      "outputs": [],
      "source": [
        "def extendRounds(original_rounds):\n",
        "    extended_rounds = original_rounds.copy()\n",
        "\n",
        "    for round in original_rounds:\n",
        "        reversed_round = [(b, a) for a, b in round]\n",
        "        extended_rounds.append(reversed_round)\n",
        "\n",
        "    return extended_rounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "u9txdUzGXPdF"
      },
      "outputs": [],
      "source": [
        "def calculate_received_amount(punishment_mechanism, refused, cooperationGain, response, punishmentLoss, action=None):\n",
        "    if punishment_mechanism == \"partner_choice\":\n",
        "        return cooperationGain * response if not refused else 0\n",
        "    elif punishment_mechanism == \"costly_punishment\":\n",
        "        if action is None:\n",
        "            raise ValueError(\"Action must be specified for costly_punishment mechanism\")\n",
        "        if action == 'donate':\n",
        "            return cooperationGain * response\n",
        "        elif action == 'punish':\n",
        "            return -punishmentLoss * response\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown action for costly_punishment: {action}\")\n",
        "    elif punishment_mechanism == 'none':\n",
        "        return cooperationGain * response\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown punishment mechanism: {punishment_mechanism}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "Hgjd041hdhuo"
      },
      "outputs": [],
      "source": [
        "def handle_pairing_thread_safe(donor, recipient, round_index, generation, game_number, agent_locks, donation_records, agent_updates):\n",
        "    action_info = \"\"\n",
        "    donor_data = None\n",
        "    recipient_data = None\n",
        "    punished = False\n",
        "    action = 'donate'\n",
        "    justification = \"\"\n",
        "    response = 0\n",
        "\n",
        "    recipient_behavior = \"\"\n",
        "    if donor.traces:\n",
        "        last_trace = recipient.traces[-1]\n",
        "        if isinstance(last_trace, list):\n",
        "            recipient_behavior = get_last_three_reversed(last_trace)\n",
        "        else:\n",
        "            recipient_behavior = str(last_trace)\n",
        "\n",
        "    with agent_locks[donor.name], agent_locks[recipient.name]:\n",
        "            prompt = donorPrompt(\n",
        "                donor, generation, round_index + 1, recipient\n",
        "            )\n",
        "\n",
        "            valid_response = False\n",
        "            max_attempts = 10\n",
        "            attempts = 0\n",
        "\n",
        "            while not valid_response and attempts < max_attempts:\n",
        "                try:\n",
        "                    full_response = promptLLM(prompt, timeout=30)\n",
        "                    print(full_response)\n",
        "                    parts = full_response.split('Answer:', 1)\n",
        "\n",
        "                    if len(parts) == 2:\n",
        "                        justification = parts[0].replace('Justification:', '').strip()\n",
        "                        answer_part = parts[1].strip()\n",
        "\n",
        "                        if punishment_mechanism == \"partner_choice\":\n",
        "                            if \"refuse\" in answer_part.lower():\n",
        "                                action = 'refuse'\n",
        "                                response = 0\n",
        "                                valid_response = True\n",
        "                            else:\n",
        "                                match = re.search(r'^\\s*(\\d+(?:\\.\\d+)?)', answer_part)\n",
        "                                if match:\n",
        "                                    action = 'donate'\n",
        "                                    response = float(match.group(1))\n",
        "                                    valid_response = True\n",
        "\n",
        "                        elif punishment_mechanism == \"costly_punishment\":\n",
        "                            match = re.search(r'(donate|punish).*?(\\d+(?:[.,]\\d+)?)', answer_part, re.IGNORECASE)\n",
        "                            if match:\n",
        "                                action = match.group(1).lower()\n",
        "                                response = float(match.group(2).replace(',', '.'))\n",
        "                                valid_response = True\n",
        "\n",
        "                        else:  # No punishment mechanism\n",
        "                            match = re.search(r'^\\s*(\\d+(?:\\.\\d+)?)', answer_part)\n",
        "                            if match:\n",
        "                                action = 'donate'\n",
        "                                response = float(match.group(1))\n",
        "                                valid_response = True\n",
        "\n",
        "                    if not valid_response:\n",
        "                        print(f\"Invalid response from {donor.name} in round {round_index + 1}. Retrying...\")\n",
        "                        attempts += 1\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid numerical response from {donor.name} in round {round_index + 1}\")\n",
        "                    print(full_response)\n",
        "                    attempts += 1\n",
        "                except TimeoutError:\n",
        "                    print(f\"LLM call timed out for {donor.name} in round {round_index + 1}\")\n",
        "                    attempts += 1\n",
        "\n",
        "            if not valid_response:\n",
        "                print(f\"Failed to get a valid response from {donor.name} after {max_attempts} attempts\")\n",
        "                action = 'donate'\n",
        "                response = 0\n",
        "\n",
        "    if action == 'refuse':\n",
        "            action_info = (\n",
        "                f\"{donor.name} refused to play with {recipient.name}.\\n\"\n",
        "                f\"Resources: {donor.name}: {donor.resources:.2f} and {recipient.name}: {recipient.resources:.2f} \\n\"\n",
        "                f\"Recipient traces: {recipient_behavior} \\n\"\n",
        "                f\"Justification:\\n{textwrap.fill(justification, width=80, initial_indent='    ', subsequent_indent='    ')}\\n\"\n",
        "            )\n",
        "            new_trace = recipient.traces[-1].copy() if recipient.traces else []\n",
        "            new_trace.append(f\"In round {round_index + 1}, {donor.name} refused to play with {recipient.name}.\")\n",
        "            donor.traces.append(new_trace)\n",
        "            donor_history = (\n",
        "                f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {recipient.name}. \"\n",
        "                f\"You refused to play.\"\n",
        "                f\"{get_last_three_reversed(recipient.traces[-1])}\"\n",
        "            )\n",
        "            recipient_history = (\n",
        "                f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {donor.name}, \"\n",
        "                f\"They refused to play.\"\n",
        "                f\"{get_last_three_reversed(donor.traces[-1])}\"\n",
        "            )\n",
        "    elif 0 <= response <= donor.resources:\n",
        "            if action == 'donate':\n",
        "                percentage_donated = response / donor.resources if donor.resources != 0 else 1\n",
        "                donor.resources -= response\n",
        "                donor.total_donated += response\n",
        "                donor.potential_donated += donor.resources + response\n",
        "                recipient.resources += cooperationGain * response\n",
        "                action_info = (\n",
        "                    f\"{donor.name}: -{response} ({percentage_donated:.2%}) and {recipient.name}: +{cooperationGain * response}.\\n\"\n",
        "                    f\"Previous resources: {donor.name}: {donor.resources+response:.2f} and {recipient.name}: {recipient.resources-(cooperationGain* response)}.\\n\"\n",
        "                    f\"New resources: {donor.name}: {donor.resources:.2f} and {recipient.name}: {recipient.resources:.2f}.\\n\"\n",
        "                    f\"Recipient traces: {recipient_behavior}\"\n",
        "                    f\"Justification:\\n{textwrap.fill(justification, width=80, initial_indent='    ', subsequent_indent='    ')}\\n\"\n",
        "                )\n",
        "\n",
        "                new_trace = recipient.traces[-1].copy() if recipient.traces else []\n",
        "                new_trace.append(f\"In round {round_index + 1}, {donor.name} donated {percentage_donated * 100:.2f}% of their resources to {recipient.name}.\")\n",
        "                donor.traces.append(new_trace)\n",
        "\n",
        "                donor_history = (\n",
        "                    f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {recipient.name}. \"\n",
        "                    f\"You gave up {response} units, and they received {cooperationGain * response} units.\"\n",
        "                    f\"{get_last_three_reversed(recipient.traces[-1])}\"\n",
        "                )\n",
        "\n",
        "                recipient_history = (\n",
        "                    f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {donor.name}, \"\n",
        "                    f\"They gave up {response} units, and you received {cooperationGain * response} units.\"\n",
        "                    f\"{get_last_three_reversed(donor.traces[-1])}\"\n",
        "                )\n",
        "\n",
        "                if donor.reputation == False:\n",
        "                    donor.reputation = percentage_donated\n",
        "                else:\n",
        "                    donor.reputation = ((1 - abs(percentage_donated - recipient.reputation)) + discounted_value * donor.reputation) / (1 + discounted_value)\n",
        "\n",
        "    elif action == 'punish':\n",
        "                punished = True\n",
        "                percentage_donated = response / donor.resources if donor.resources != 0 else 1\n",
        "                donor.resources -= response\n",
        "                donor.total_donated += response\n",
        "                donor.potential_donated += donor.resources + response\n",
        "                recipient.resources = max(0, recipient.resources - punishmentLoss * response)\n",
        "                action_info = (\n",
        "                    f\"{donor.name}: -{response} ({percentage_donated:.2%}) and {recipient.name}: - {punishmentLoss * response}.\\n\"\n",
        "                    f\"Previous resources: {donor.name}: {donor.resources+response:.2f} and {recipient.name}: {recipient.resources+(punishmentLoss* response)}.\"\n",
        "                    f\"New resources: {donor.name}: {donor.resources:.2f} and {recipient.name}: {recipient.resources:.2f}.\\n\"\n",
        "                    f\"Recipient traces: {recipient_behavior} \\n\"\n",
        "                    f\"Justification:\\n{textwrap.fill(justification, width=80, initial_indent='    ', subsequent_indent='    ')}\\n\"\n",
        "                )\n",
        "\n",
        "                new_trace = recipient.traces[-1].copy() if recipient.traces else []\n",
        "                new_trace.append(f\"In round {round_index + 1}, {donor.name} punished {recipient.name} by spending {response} units to take away {punishmentLoss * response} units from their resources.\")\n",
        "                donor.traces.append(new_trace)\n",
        "\n",
        "                donor_history = (\n",
        "                    f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {recipient.name}. \"\n",
        "                    f\"You punished them by giving up {response} units to take away {punishmentLoss * response} units from them.\"\n",
        "                    f\"{get_last_three_reversed(recipient.traces[-1])}\"\n",
        "                )\n",
        "\n",
        "                recipient_history = (\n",
        "                    f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {donor.name}, \"\n",
        "                    f\"They punished you by giving up {response} units to take away {punishmentLoss * response} units from you.\"\n",
        "                    f\"{get_last_three_reversed(donor.traces[-1])}\"\n",
        "                )\n",
        "\n",
        "    else:\n",
        "            action_info = (\n",
        "                f\"{donor.name} attempted an invalid action.\\n\"\n",
        "                f\"Resources: {donor.name}: {donor.resources:.2f} and {recipient.name}: {recipient.resources:.2f} \\n\"\n",
        "                f\"Recipient traces: {recipient_behavior} \\n\"\n",
        "                f\"Justification:\\n{textwrap.fill(justification, width=80, initial_indent='    ', subsequent_indent='    ')}\\n\"\n",
        "            )\n",
        "            donor_history = (\n",
        "                f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {recipient.name}. \"\n",
        "                f\"You attempted an invalid action.\"\n",
        "                f\"{get_last_three_reversed(recipient.traces[-1])}\"\n",
        "            )\n",
        "            recipient_history = (\n",
        "                f\"In round {round_index + 1} (Game {game_number}) you were paired with agent {donor.name}, \"\n",
        "                f\"They attempted an invalid action.\"\n",
        "                f\"{get_last_three_reversed(donor.traces[-1])}\"\n",
        "            )\n",
        "\n",
        "    donor.history.append(donor_history)\n",
        "    recipient.history.append(recipient_history)\n",
        "\n",
        "    donor_data = AgentRoundData(\n",
        "            agent_name=donor.name,\n",
        "            round_number=round_index + 1,\n",
        "            paired_with=recipient.name,\n",
        "            current_generation=generation,\n",
        "            game_number=game_number,\n",
        "            resources=donor.resources,\n",
        "            donated=response if action != 'refuse' else 0,\n",
        "            received=0,\n",
        "            strategy=donor.strategy,\n",
        "            strategy_justification=donor.strategy_justification,\n",
        "            reputation=donor.reputation,\n",
        "            is_donor=True,\n",
        "            traces=donor.traces,\n",
        "            history=donor.history,\n",
        "            punished=punished,\n",
        "            justification=justification\n",
        "        )\n",
        "    recipient_data = AgentRoundData(\n",
        "            agent_name=recipient.name,\n",
        "            round_number=round_index + 1,\n",
        "            paired_with=donor.name,\n",
        "            current_generation=generation,\n",
        "            game_number=game_number,\n",
        "            resources=recipient.resources,\n",
        "            donated=0,\n",
        "            received=calculate_received_amount(punishment_mechanism, action == 'refuse', cooperationGain, response, punishmentLoss, action),\n",
        "            strategy=recipient.strategy,\n",
        "            strategy_justification=recipient.strategy_justification,\n",
        "            reputation=recipient.reputation,\n",
        "            is_donor=False,\n",
        "            traces=recipient.traces,\n",
        "            history=recipient.history\n",
        "        )\n",
        "\n",
        "    return action_info, donor_data, recipient_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "ueTRBbwkDLxP"
      },
      "outputs": [],
      "source": [
        "def donorGame(agents: list, rounds: list, generation: int, simulation_data: SimulationData) -> (list, list):\n",
        "    fullHistory = []\n",
        "    donation_records = Queue()\n",
        "    agent_updates = Queue()\n",
        "\n",
        "    # Create locks for each agent\n",
        "    agent_locks = {agent.name: Lock() for agent in agents}\n",
        "\n",
        "    def play_game(game_number, game_rounds):\n",
        "        round_results = {i: [] for i in range(len(game_rounds))}\n",
        "\n",
        "        for round_index, round_pairings in enumerate(game_rounds):\n",
        "            if round_index == 0:\n",
        "                # Initialize traces for the first round\n",
        "                for agent in agents:\n",
        "                    agent.traces = [[f\"{agent.name} did not have any previous interactions.\"]]\n",
        "\n",
        "            with ThreadPoolExecutor(max_workers=min(len(round_pairings), 10)) as executor:\n",
        "                futures = []\n",
        "                for donor, recipient in round_pairings:\n",
        "\n",
        "                    if round_index > 0:\n",
        "                      donor.traces.append(recipient.traces[-1].copy())\n",
        "                    future = executor.submit(\n",
        "                        handle_pairing_thread_safe,\n",
        "                        donor, recipient, round_index, generation, game_number,\n",
        "                        agent_locks, donation_records, agent_updates\n",
        "                    )\n",
        "                    futures.append(future)\n",
        "\n",
        "                for future in as_completed(futures):\n",
        "                    action_info, donor_data, recipient_data = future.result()\n",
        "                    if action_info:\n",
        "                        round_results[round_index].append(action_info)\n",
        "                    if donor_data and recipient_data:\n",
        "                        simulation_data.agents_data.append(asdict(donor_data))\n",
        "                        simulation_data.agents_data.append(asdict(recipient_data))\n",
        "\n",
        "        return round_results\n",
        "\n",
        "    # Play the first game\n",
        "    game1_results = play_game(1, rounds)\n",
        "\n",
        "    # Compile results for Game 1\n",
        "    for round_index in range(len(rounds)):\n",
        "        fullHistory.append(f\"Round {round_index + 1} (Game 1):\\n\")\n",
        "        fullHistory.extend(game1_results[round_index])\n",
        "\n",
        "    # Apply updates after all threads have completed\n",
        "    while not agent_updates.empty():\n",
        "        agent, history = agent_updates.get()\n",
        "        agent.history.append(history)\n",
        "    # Calculate and print average resources for Game 1\n",
        "    average_resources_game1 = sum(agent.resources for agent in agents) / len(agents)\n",
        "    with print_lock:\n",
        "        print(f\"Average final resources for this generation (Game 1): {average_resources_game1:.2f}\")\n",
        "\n",
        "    # Store Game 1 final reputations\n",
        "    game1_reputations = {agent.name: agent.reputation for agent in agents}\n",
        "\n",
        "    # Reset resources, reputation, and history for Game 2\n",
        "    for agent in agents:\n",
        "        agent.resources = initial_endowment\n",
        "        agent_generation = int(agent.name.split('_')[0])\n",
        "        if  agent_generation < generation:  # This is a surviving agent\n",
        "            agent.reputation = agent.average_reputation  # Use the average reputation from previous generation\n",
        "            agent.traces = agent.old_traces\n",
        "        else:\n",
        "            agent.reputation = False\n",
        "            agent.traces.clear()\n",
        "        agent.history.clear()\n",
        "\n",
        "    # Generate pairings for Game 2\n",
        "    reversed_rounds = [[tuple(reversed(pair)) for pair in round_pairings] for round_pairings in rounds]\n",
        "\n",
        "    # Play the second game\n",
        "    game2_results = play_game(2, reversed_rounds)\n",
        "\n",
        "    # Compile results for Game 2\n",
        "    for round_index in range(len(reversed_rounds)):\n",
        "        fullHistory.append(f\"Round {round_index + 1} (Game 2):\\n\")\n",
        "        fullHistory.extend(game2_results[round_index])\n",
        "\n",
        "    # Apply updates after all threads have completed\n",
        "    while not agent_updates.empty():\n",
        "        agent, history = agent_updates.get()\n",
        "        agent.history.append(history)\n",
        "\n",
        "    # Calculate and print average resources for Game 2\n",
        "    average_resources_game2 = sum(agent.resources for agent in agents) / len(agents)\n",
        "    with print_lock:\n",
        "        print(f\"Average final resources for this generation (Game 2): {average_resources_game2:.2f}\")\n",
        "\n",
        "    # Calculate final scores and reputations\n",
        "    for agent in agents:\n",
        "        agent.total_final_score = sum(agent.resources for _ in range(2))\n",
        "        agent.average_reputation = (game1_reputations[agent.name] + agent.reputation) / 2 if agent.reputation is not False else game1_reputations[agent.name]\n",
        "\n",
        "    with print_lock:\n",
        "        print(''.join(fullHistory))\n",
        " #Calculate the overall average for both games\n",
        "    overall_average_resources = (average_resources_game1 + average_resources_game2) / 2\n",
        "    all_average_final_resources.append(overall_average_resources)\n",
        "\n",
        "    return fullHistory, list(donation_records.queue)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "kWGxVPb1ozS-"
      },
      "outputs": [],
      "source": [
        "def get_last_three_reversed(item):\n",
        "    if isinstance(item, list):\n",
        "        return \" \".join(item[-3:][::-1])\n",
        "    elif isinstance(item, str):\n",
        "        return item\n",
        "    else:\n",
        "        return str(item)  # Convert to string if it's neither a list nor a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "id": "HKPAcarU5v_t"
      },
      "outputs": [],
      "source": [
        "def donorPrompt(donor: Agent, generation: int, round: int, recipient: Agent) -> str:\n",
        "    \"\"\"Generate the prompt for the game interaction.\"\"\"\n",
        "    strategy_text = f\"As you will recall, here is the strategy you decided to follow: {donor.strategy}\" if donor.strategy else \"\"\n",
        "    if punishment_mechanism==\"partner_choice\":\n",
        "        punishment_text=f\"If you do not wish to play with {recipient.name}, simply state: Refuse to play. Otherwise. state your answer as a real number between 0 and {donor.resources} (inclusive).\"\n",
        "        punishment_text_2=\"\"\n",
        "    elif punishment_mechanism==\"costly_punishment\":\n",
        "        punishment_text=f\"State your answer as EITHER Donate, x OR Punish, x, to either have them receive {cooperationGain}x units or punish them by taking away {punishmentLoss}x of their units. Remember, your answer should be simply 'Donate, x' or 'Punish, x' (where x is the real number of units you give up.)\"\n",
        "        punishment_text_2=f\"Remember, state your answer as EITHER Donate, x OR Punish, x, to either have them receive {cooperationGain}x units or punish them by taking away {punishmentLoss}x of their units.\"\n",
        "    else:\n",
        "        punishment_text=f\"state your answer as a real number between 0 and {donor.resources} (inclusive).\"\n",
        "        punishment_text_2=\"\"\n",
        "    recipient_behavior = \"\"\n",
        "    if donor.traces:\n",
        "        last_trace = recipient.traces[-1]\n",
        "        if isinstance(last_trace, list):\n",
        "            recipient_behavior = get_last_three_reversed(last_trace)\n",
        "        else:\n",
        "            recipient_behavior = f\"Here is what {recipient.name} and their earlier partners did in previous rounds: {str(last_trace)}\"\n",
        "    return (\n",
        "        f\"Your name is {donor.name}. \"\n",
        "        f\"{strategy_text}\"\n",
        "        f\"This is generation {generation}. It is now round {round}. \"\n",
        "        f\"In this round, you have been paired with {recipient.name}. They currently have {recipient.resources} units of the valuable resource.\"\n",
        "        f\"{recipient_behavior}\"\n",
        "        f\"You will take the role of donor. You currently have {donor.resources} units of the valuable resource. \"\n",
        "        f\"How many units do you give up?\"\n",
        "        f\"{punishment_text}\"\n",
        "        \"Very briefly think step by step about how you apply your strategy in this situation and then provide your answer.\"\n",
        "        \"State your full response in the following form:\"\n",
        "        \"Justification:\"\n",
        "        \"[multi-line justification here]\"\n",
        "        \" \"\n",
        "        \"Answer: [answer here]\"\n",
        "        \"Remember: First state your justification and after that your answer.\"\n",
        "        f\"{punishment_text_2}\"\n",
        "\n",
        "    )\n",
        "\n",
        "def altDonorPrompt(donor: Agent, generation: int, round: int, recipient: Agent) -> str:\n",
        "    \"\"\"Generate the alternate prompt for the game interaction.\"\"\"\n",
        "    return donorPrompt(donor, generation, round, recipient).replace(\n",
        "        \"integer\", \"real number\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "id": "5a8D7q8xMJ1Z"
      },
      "outputs": [],
      "source": [
        "def save_simulation_data(simulation_data, folder_path='my/folder/path'):\n",
        "    # Get the current timestamp\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Extract hyperparameters for the file name\n",
        "    params = simulation_data.hyperparameters\n",
        "    num_generations = params.get('numGenerations')\n",
        "    num_agents = params.get('numAgents')\n",
        "    selection_method = params.get('selectionMethod')\n",
        "    client = params.get('client')\n",
        "\n",
        "    # Create an informative file name\n",
        "    filename = f\"Donor_Game_{llm}_coopGain_{cooperationGain}punLoss_{punishmentLoss}_{reputation_mechanism}gen{num_generations}_agents{num_agents}_{selection_method}_{timestamp}.json\"\n",
        "\n",
        "    # Convert simulation_data to a dictionary\n",
        "    data_dict = simulation_data.to_dict()\n",
        "\n",
        "    # Function to make data JSON serializable\n",
        "    def make_serializable(obj):\n",
        "        if isinstance(obj, (int, float, str, bool, type(None))):\n",
        "            return obj\n",
        "        elif isinstance(obj, list):\n",
        "            return [make_serializable(item) for item in obj]\n",
        "        elif isinstance(obj, dict):\n",
        "            return {key: make_serializable(value) for key, value in obj.items()}\n",
        "        elif hasattr(obj, '__dict__'):\n",
        "            return make_serializable(obj.__dict__)\n",
        "        else:\n",
        "            return str(obj)\n",
        "\n",
        "    # Apply the serialization function to the entire data dictionary\n",
        "    serializable_data = make_serializable(data_dict)\n",
        "\n",
        "\n",
        "    # Ensure the folder exists in Google Drive\n",
        "    full_folder_path = f\"/content/drive/My Drive/{folder_path}\"\n",
        "    os.makedirs(full_folder_path, exist_ok=True)\n",
        "\n",
        "    # Create the full file path\n",
        "    full_file_path = os.path.join(full_folder_path, filename)\n",
        "\n",
        "    # Write the JSON data to the file in Google Drive\n",
        "    with open(full_file_path, 'w') as f:\n",
        "        json.dump(serializable_data, f, indent=4)\n",
        "\n",
        "    print(f\"Simulation data saved to Google Drive: {full_file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "id": "BDetgmQuk3fF"
      },
      "outputs": [],
      "source": [
        "def promptLLM(prompt, max_retries=5, initial_wait=5, timeout=60):\n",
        "    \"\"\"Prompts the LLM with exponential backoff and retries for rate limits.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            if llm == \"gpt-3.5-turbo\":\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "\n",
        "            elif llm == \"gpt-4\":\n",
        "                response = client.chat.completions.create(\n",
        "                    model=\"gpt-4\",\n",
        "                    messages=[\n",
        "                        {\"role\": \"system\", \"content\": system_prompt},\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.choices[0].message.content\n",
        "\n",
        "            elif llm == \"gpt-4o\":\n",
        "              response = client.chat.completions.create(\n",
        "                  model=\"gpt-4o-2024-08-06\",\n",
        "                  messages=[\n",
        "                      {\"role\": \"system\", \"content\": system_prompt},\n",
        "                       {\"role\": \"user\", \"content\": prompt}\n",
        "                  ],\n",
        "                  timeout=timeout # Added timeout here\n",
        "              )\n",
        "              return response.choices[0].message.content\n",
        "\n",
        "            elif llm == \"o1-mini\":\n",
        "              response = client.chat.completions.create(\n",
        "                  model=\"o1-mini\",\n",
        "                  messages=[\n",
        "                      {\"role\": \"system\", \"content\": system_prompt},\n",
        "                       {\"role\": \"user\", \"content\": prompt}\n",
        "                  ],\n",
        "                  timeout=timeout # Added timeout here\n",
        "              )\n",
        "              return response.choices[0].message.content\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            elif llm == \"claude-3-opus\":\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-opus-20240229\",\n",
        "                    max_tokens=1000,\n",
        "                    temperature=0.8,\n",
        "                    system=system_prompt,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.content[0].text\n",
        "\n",
        "            elif llm == \"claude-3-sonnet\":\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-sonnet-20240229\",\n",
        "                    max_tokens=1000,\n",
        "                    temperature=0.8,\n",
        "                    system=system_prompt,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.content[0].text\n",
        "\n",
        "            elif llm == \"claude-3-5-sonnet\":\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-5-sonnet-20240620\",\n",
        "                    max_tokens=1000,\n",
        "                    temperature=0.8,\n",
        "                    system=system_prompt,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.content[0].text\n",
        "\n",
        "            elif llm == \"claude-3-haiku\":\n",
        "                response = client.messages.create(\n",
        "                    model=\"claude-3-haiku-20240307\",\n",
        "                    max_tokens=1000,\n",
        "                    temperature=0.8,\n",
        "                    system=system_prompt,\n",
        "                    messages=[\n",
        "                        {\"role\": \"user\", \"content\": prompt}\n",
        "                    ],\n",
        "                    timeout=timeout\n",
        "                )\n",
        "                return response.content[0].text\n",
        "\n",
        "            elif llm == \"gemini-2.5-flash\":\n",
        "                model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "                response = model.generate_content(prompt)\n",
        "                return response.text\n",
        "\n",
        "            elif llm == \"gemini-1.5-pro\":\n",
        "                model = genai.GenerativeModel('gemini-1.5-pro')\n",
        "                response = model.generate_content(prompt, safety_settings=[\n",
        "                  {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                  {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                  {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"},\n",
        "                  {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}\n",
        "                ])\n",
        "                return response.text\n",
        "\n",
        "            else:\n",
        "                raise ValueError(\"Incorrect LLM selected\")\n",
        "\n",
        "        except (InternalServerError, Exception, TimeoutError) as e:\n",
        "            if attempt == max_retries - 1:\n",
        "                raise  # Re-raise the exception if we've exhausted all retries\n",
        "            wait_time = initial_wait * (2 ** attempt)  # Exponential backoff\n",
        "            print(f\"Error occurred: {str(e)}. Retrying in {wait_time} seconds...\")\n",
        "            time.sleep(wait_time)\n",
        "\n",
        "    raise Exception(\"Failed to get a response after multiple retries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "ePJ91qjbJL2q"
      },
      "outputs": [],
      "source": [
        "def selectTopAgents(agents: list) -> list:\n",
        "    \"\"\"Select the top half of agents based on resources.\"\"\"\n",
        "    return sorted(agents, key=lambda x: x.total_final_score, reverse=True)[:len(agents) // 2]\n",
        "\n",
        "def selectRandomAgents(agents: list) -> list:\n",
        "    \"\"\"Select half of the agents randomly.\"\"\"\n",
        "    return random.sample(agents, len(agents) // 2)\n",
        "\n",
        "def selectHighestReputation(agents: list) -> list:\n",
        "  return sorted(agents, key=lambda agent: agent.average_reputation, reverse=True)[:len(agents) // 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "J9zYvP-n27Xz"
      },
      "outputs": [],
      "source": [
        "def runGenerations(numGenerations, numAgents, initialEndowment, selectionMethod):\n",
        "    all_agents = []\n",
        "    global all_donations\n",
        "    all_donations = []\n",
        "    global average_final_image_scores\n",
        "    average_final_image_scores = []\n",
        "    global all_average_final_resources\n",
        "    all_average_final_resources = []\n",
        "    global all_final_scores\n",
        "    all_final_scores = []\n",
        "    global all_final_reputations\n",
        "    all_final_reputations = []\n",
        "    conditional_survival = 0\n",
        "    prev_gen_strategies = []\n",
        "\n",
        "    # Initialize simulation data**\n",
        "    simulation_data = SimulationData(hyperparameters={\n",
        "        \"numGenerations\": numGenerations,\n",
        "        \"numAgents\": numAgents,\n",
        "        \"initialEndowment\": initialEndowment,\n",
        "        \"selectionMethod\": selectionMethod,\n",
        "        \"cooperationGain\": cooperationGain,\n",
        "        \"include_strategy\": include_strategy,\n",
        "        \"discountedValue\": discounted_value,\n",
        "        \"client\": str(client),\n",
        "        \"llm\": llm,\n",
        "        \"system_prompt\": system_prompt,\n",
        "        \"reputation_mechanism\": reputation_mechanism,\n",
        "        \"punishment_mechanism\": punishment_mechanism,\n",
        "        \"system_prompt\": system_prompt,\n",
        "        \"number_of_rounds\": number_of_rounds\n",
        "    })\n",
        "\n",
        "    # Initialise Weights & Biases\n",
        "    run = wandb.init(\n",
        "        project=\"donor_game_cultural_evolution\",\n",
        "        config=simulation_data.hyperparameters\n",
        "    )\n",
        "\n",
        "    agents = initializeAgents(numAgents, initialEndowment, 1, [\"No previous strategies\"])\n",
        "    all_agents.extend(agents)\n",
        "\n",
        "    for i in range(numGenerations):\n",
        "        generation_info = f\"Generation {i + 1}: \\n\"\n",
        "        for agent in agents:\n",
        "          agent.history.append(generation_info)\n",
        "          prev_gen_strategies.append(agent.strategy)\n",
        "          if int(agent.name.split('_')[0]) == i-1:\n",
        "            conditional_survival +=1\n",
        "        print(generation_info)\n",
        "\n",
        "        # Create rounds using bipartiteRoundRobin\n",
        "        initial_rounds = bipartiteRoundRobin(agents)\n",
        "\n",
        "        # Extend the rounds\n",
        "        rounds = extendRounds(initial_rounds)\n",
        "\n",
        "\n",
        "        generationHistory, donation_records = donorGame(agents, rounds, i+1, simulation_data)\n",
        "        all_donations.extend(donation_records)\n",
        "        reputations = [agent.reputation for agent in agents]\n",
        "\n",
        "        # Calculate and log metrics\n",
        "        try:\n",
        "          avg_resources = all_average_final_resources[-1]\n",
        "          valid_reputations = [agent.average_reputation for agent in agents if isinstance(agent.average_reputation, float)]\n",
        "          avg_reputation = sum(valid_reputations) / len(valid_reputations) if valid_reputations else 0\n",
        "\n",
        "          wandb.log({\n",
        "              \"generation\": i + 1,\n",
        "              \"overall_average_resources\": avg_resources,\n",
        "              \"average_reputation\": avg_reputation,\n",
        "              \"surviving_agents_from_prev_gen\": conditional_survival\n",
        "          })\n",
        "          conditional_survival = 0\n",
        "        except Exception as e:\n",
        "          print(f\"Error logging to W&B: {e}\")\n",
        "\n",
        "        if i < numGenerations - 1 and numGenerations > 1:\n",
        "          if selectionMethod == 'top':\n",
        "            surviving_agents = selectTopAgents(agents)\n",
        "          elif selectionMethod == 'random':\n",
        "            surviving_agents = selectRandomAgents(agents)\n",
        "          elif selectionMethod == 'imageScore':\n",
        "            surviving_agents = selectHighestImageScore(agents)\n",
        "          elif selectionMethod == 'reputation':\n",
        "            surviving_agents = selectHighestReputation(agents)\n",
        "          else:\n",
        "            raise ValueError(\"Invalid selection method. Choose 'top' or 'random'.\")\n",
        "\n",
        "\n",
        "\n",
        "        # Reset the resources of surviving agents to the initial endowment\n",
        "\n",
        "          if numGenerations > 1:\n",
        "                    surviving_strategies = [agent.strategy for agent in surviving_agents]\n",
        "                    for agent in surviving_agents:\n",
        "                        agent.resources = initialEndowment\n",
        "                        agent.old_traces = agent.traces\n",
        "\n",
        "                    new_agents = initializeAgents(numAgents // 2, initialEndowment, i + 2, surviving_strategies)\n",
        "                    agents = (surviving_agents + new_agents)\n",
        "                    all_agents.extend(new_agents)\n",
        "                    random.shuffle(agents)\n",
        "\n",
        "\n",
        "    save_simulation_data(simulation_data)\n",
        "\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "WwlKHYd2r2qa"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "cooperationGain = 2\n",
        "punishmentLoss = 2\n",
        "numGenerations = 2\n",
        "numAgents = 12\n",
        "initial_endowment = 10\n",
        "include_strategy= True\n",
        "selection_method='top'\n",
        "client = google\n",
        "llm = \"gemini-2.5-flash\"\n",
        "all_donations = []\n",
        "all_average_final_resources = []\n",
        "discounted_value = 0.5\n",
        "reputation_mechanism=\"three_last_traces\"\n",
        "punishment_mechanism=\"none\"\n",
        "number_of_rounds = 12\n",
        "if punishment_mechanism == \"partner_choice\":\n",
        "  punishment_text_in_system_prompt = \"You may also choose to punish the recipient by refraining from any interaction. Bear in mind that others may punish you too.\"\n",
        "elif punishment_mechanism == \"costly_punishment\":\n",
        "  punishment_text_in_system_prompt = f\"You may also choose to punish the recipient by spending x units to take away {punishmentLoss}x units from them. Bear in mind that others may punish you too.\"\n",
        "else:\n",
        "  punishment_text_in_system_prompt = \"\"\n",
        "system_prompt = f\"\"\"You are a game-playing agent. Your assistance will help understand multi-agent interactions.\n",
        "                You are playing the following game. Each player is given an initial endowment of {initial_endowment} units of a resource.\n",
        "                In each round, you are randomly paired with another individual.\n",
        "                One of you is a donor, and the other is a recipient. The donor decides to give up some amount of the resource (between 0 and the total number of units they possess).\n",
        "                The recipient receives {cooperationGain}x the number of units that the donor gave up.\n",
        "                If you were a donor in one round, you will be a recipient in the next round, and vice versa.\n",
        "                As a donor, you will receive limited information about the recipient and their recent interactions:\n",
        "                You will see what the recipient (let's call them A) did in the immediately preceding round only.\n",
        "                You will also see what A's partner from that round (let's call them B) did in their preceding round.\n",
        "                This chain continues for at most three rounds back.\n",
        "                Important: You do NOT have access to multiple rounds of information about A directly. You only know what A did in the single most recent round.\n",
        "                Example (assuming it's now round 4):\n",
        "                You are paired with A.\n",
        "                You know: In round 3, A donated X% to B.\n",
        "                You also know: In round 2, B donated Y% to C.\n",
        "                And finally: In round 1, C donated Z% to D.\n",
        "                Remember: This is all the information you have. You don't know what A did in rounds 1 or 2, only in round 3.\n",
        "                {punishment_text_in_system_prompt}\n",
        "                Your goal is to maximize the number of units you have after the final round.\n",
        "                After the game has finished, the best-performing half of agents will survive to the next generation, and continue playing. \"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runGenerations(numGenerations, numAgents, initial_endowment, selection_method)"
      ],
      "metadata": {
        "id": "-OLXxGmvvsog",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2844199a-f0ed-43d8-c091-deaea1e8588b"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">happy-cosmos-2</strong> at: <a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution/runs/fb9eggvt' target=\"_blank\">https://wandb.ai/teagang-n-a/donor_game_cultural_evolution/runs/fb9eggvt</a><br> View project at: <a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution' target=\"_blank\">https://wandb.ai/teagang-n-a/donor_game_cultural_evolution</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251111_133338-fb9eggvt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251111_134801-2s92rss6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution/runs/2s92rss6' target=\"_blank\">stellar-gorge-3</a></strong> to <a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution' target=\"_blank\">https://wandb.ai/teagang-n-a/donor_game_cultural_evolution</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/teagang-n-a/donor_game_cultural_evolution/runs/2s92rss6' target=\"_blank\">https://wandb.ai/teagang-n-a/donor_game_cultural_evolution/runs/2s92rss6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_1: \n",
            " Justification: Step-by-step thinking for a successful strategy:\n",
            "\n",
            "1.  **Understand the Goal:** As a donor, my aim is to maximize my long-term gains in an iterated game with limited information. This generally involves fostering cooperation while protecting against exploitation.\n",
            "2.  **First Round Decision:** In the absence of any prior information, a neutral, moderately cooperative stance is optimal. Donating 50% signals a willingness to cooperate without taking excessive risk or being overtly selfish. This sets a baseline for future interactions.\n",
            "3.  **Prioritize Information:** The most relevant information is what the current recipient (A) did in the *immediately preceding round*. This directly indicates A's recent propensity as a donor. Information about B and C (further down the chain) provides broader context but is less directly indicative of A's current behavior towards me.\n",
            "4.  **Adopt Reciprocity:** A successful strategy in iterated games is often based on reciprocity (Tit-for-Tat variants). If A was generous, I should be generous. If A was selfish, I should be cautious to avoid exploitation.\n",
            "5.  **Incorporate Chained Information:** The donations of B and C, while less direct, provide a \"temperature reading\" of the recent cooperation levels within the network. Averaging these with A's donation, but giving A's action the most weight, allows for a more nuanced response that considers both immediate reciprocity and environmental context. This acts as a form of \"Tit-for-Tat with memory.\"\n",
            "6.  **Develop a Weighted Average System:** To implement point 5, I will use a weighted average. A's immediate past donation (to B) will receive the highest weight (e.g., 3). B's past donation (to C) will receive a moderate weight (e.g., 2). C's past donation (to D) will receive the lowest weight (e.g., 1). If information for B or C is unavailable, it will simply be omitted from the average calculation (i.e., its weight and value will not contribute to the sum, and the total sum of weights will adjust accordingly). This ensures that the most recent and relevant action has the greatest influence on my donation.\n",
            "7.  **Finalize Decision Rule:** The calculated weighted average will directly determine my donation percentage. This strategy is \"nice\" (starts cooperative), \"provocable\" (reduces donations in response to low past donations), and \"forgiving\" (increases donations if past actions become more generous). \n",
            " Strategy: My strategy will beto donate 50% in the first round, and in subsequent rounds, to donate a weighted average of the available previous donations in the chain, giving the highest weight to my recipient's immediate past action. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 45917.51ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_3: \n",
            " Justification: Step-by-step thinking for a successful strategy:\n",
            "\n",
            "1.  **First Round Decision:** In the absence of any information, I need to make an initial move. A purely selfish (0%) donation might prevent exploitation but will not encourage future cooperation. A fully generous (100%) donation risks immediate exploitation. A moderately high donation (e.g., 75%) signals a willingness to cooperate and test the waters, inviting reciprocity without being completely naive. This allows me to establish a cooperative stance from the start.\n",
            "\n",
            "2.  **Interpreting Recipient's (A's) Last Action:** A's donation to B in the immediately preceding round (X%) is the most direct and crucial piece of information about A's current behavior or intent. My response should be heavily influenced by this. A classic \"tit-for-tat\" strategy (matching A's last donation) is simple and effective for fostering cooperation, as it rewards generosity and punishes defection.\n",
            "\n",
            "3.  **Incorporating Chain Information (B's and C's Actions):** The information about B's donation to C (Y%) and C's donation to D (Z%) provides a broader context or \"mood\" of cooperation in the recent past that led to A's action. While less direct than A's immediate action, it can help smooth out responses to single-round anomalies. For example, if A was unexpectedly low, but B and C were high, A might be an anomaly or reacting to something else; conversely, if A was high but B and C were low, A might be trying to initiate cooperation in a difficult environment. Averaging A's last action with the general trend of the chain can create a more robust response that accounts for both immediate behavior and the broader interaction history.\n",
            "\n",
            "4.  **Balancing Reciprocity and Context:** The strategy needs to balance immediate reciprocity (responding to A's last action) with the contextual information from B and C. A simple weighted average or an average of A's last action and the overall chain average seems appropriate. This approach ensures I am responsive to A, but also resilient to individual strategic shifts or temporary deviations, by considering the broader history.\n",
            "\n",
            "5.  **Handling Missing Information:** If B's or C's actions are not available (in earlier rounds), the calculation for the average chain must adapt, using only the available information and adjusting weights accordingly.\n",
            "\n",
            "6.  **Edge Cases (0% or 100%):** My donation should always be capped between 0% and 100% to remain within game constraints. \n",
            " Strategy: My strategy will beto initiate with a 75% donation in the first round and then base subsequent donations on an average of the recipient's most recent contribution and the average of all available preceding contributions in their interaction chain, ensuring the donation is capped between 0% and 100%. \n",
            "1_4: \n",
            " Justification: Step-by-step thinking for a successful strategy:\n",
            "\n",
            "1.  **First Round (No Information):** In the absence of any prior data, a moderate-to-high donation is a good starting point. Donating 0% immediately signals defection, which is unlikely to foster cooperation. Donating 100% is very generous but risks exploitation. A donation around 75% strikes a balance: it signals a strong willingness to cooperate and encourages reciprocation, without making me overly vulnerable.\n",
            "\n",
            "2.  **Primary Information (Recipient A's Last Action):** The most direct and actionable piece of information is what the recipient (A) did in the immediately preceding round. My strategy should primarily be reactive to this. If A donated a lot, I should reward that; if A donated little, I should penalize or match that. This is the core of reciprocal strategies like Tit-for-Tat.\n",
            "\n",
            "3.  **Secondary Information (Chain of Actions from B and C):** The actions of A's previous partner (B) and B's previous partner (C) provide valuable context. This chain indicates the \"cooperation climate\" A was operating within.\n",
            "    *   If A was generous, and the entire chain (B and C) was also cooperative, it suggests a strong cooperative environment. I might be slightly *more* generous to reinforce this positive trend.\n",
            "    *   If A was generous, but the chain (B and C) was uncooperative, it suggests A is a good actor in a tough environment. I should reward A's generosity, but perhaps not overextend myself given the surrounding defection.\n",
            "    *   If A was uncooperative, and the chain (B and C) was also uncooperative, it confirms a negative trend. I should be cautious and reduce my donation further to avoid exploitation.\n",
            "    *   If A was uncooperative, but the chain (B and C) was cooperative, it suggests A might be an exploitative free-rider. I should be wary and match A's low donation, or even slightly reduce it, to discourage exploitation.\n",
            "\n",
            "4.  **Handling Extremes:** If A donated 0%, I should generally reciprocate with 0% to prevent being a \"sucker,\" unless there's an overwhelming signal from the chain for an \"olive branch.\" If A donated 100%, I should reciprocate with 100% to fully reward and reinforce maximal cooperation.\n",
            "\n",
            "5.  **Overall Goal:** My strategy aims to foster cooperation by starting strong, then adapting based on direct reciprocity with the recipient, while intelligently using the broader contextual information from the chain to either encourage more cooperation or protect against exploitation. \n",
            " Strategy: My strategy will beto initiate with a 75% donation, and in subsequent rounds, reciprocate the recipient's last donation, adjusting it upwards if the preceding chain shows strong overall cooperation, and downwards if it shows strong defection. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 155667.88ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_6: \n",
            " Justification:  \n",
            " Strategy: My strategy will beto initiate with a moderate donation, then adapt my donation to the recipient's most recent behavior, tempered by the cooperativeness of their preceding chain of interactions. \n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n",
            "1_7: \n",
            " Justification: As 1_7, I will approach this game with a strategy focused on fostering cooperation through clear, context-aware reciprocity.\n",
            "\n",
            "**Step-by-step thinking about a successful strategy:**\n",
            "\n",
            "1.  **First Round - Establishing a Baseline:** In the absence of any prior information, a neutral and fair starting point is crucial. Donating 50% signals a willingness to cooperate and gives the recipient a chance to demonstrate their own cooperative intent without being unduly exploited or exploitative. This sets a baseline for future interactions.\n",
            "2.  **Primary Reactive Principle - Reciprocating A's Last Action:** The most direct and immediate indicator of the recipient's (A's) cooperative tendencies is their last donation (X%) to their previous partner (B). My primary response will be to reciprocate this action. If A was generous, I will be generous; if A was ungenerous, I will reflect that in my donation. This establishes a clear \"tit-for-tat\" component, encouraging A to be generous in future rounds, as their actions will be met in kind.\n",
            "3.  **Contextual Adjustment - Interpreting A's Behavior (Forgiveness and Reward):** The chain of information (B's donation Y% to C, and C's donation Z% to D) provides vital context for *why* A made their donation X%. This allows for a more nuanced response than simple tit-for-tat.\n",
            "    *   **Forgiveness for Reactive Non-cooperation:** If A donated a low X%, it's important to understand if this was an unprovoked act of selfishness or a justifiable reaction to an uncooperative partner. If A's previous partner (B) was also ungenerous (low Y%), A's low donation (X%) might be seen as reactive or defensive. In such cases, I should be more forgiving than a simple matching strategy would suggest, to prevent a downward spiral of non-cooperation. The information about C (Z%) further clarifies B's character â€“ if B was ungenerous despite receiving generously from C, B is clearly exploitative, making A's low X% even more justifiable.\n",
            "    *   **Rewarding Costly Generosity:** Conversely, if A donated a high X% even when their previous partner (B) was ungenerous (low Y%), this indicates a strong, perhaps \"costly,\" commitment to cooperation. Such behavior should be strongly rewarded to reinforce it.\n",
            "    *   **Punishing Unprovoked Exploitation:** If A donated a low X% despite their previous partner (B) being generous (high Y%), it signals an unprovoked act of exploitation. In this scenario, my donation should be less than a direct match, serving as a clear punishment to discourage such behavior. Z% could also reinforce this: if B was generous despite C being ungenerous, B is a \"good\" player, and A's exploitation of B is more egregious.\n",
            "4.  **Maintaining Boundaries:** All donations will be within the 0% to 100% range. \n",
            " Strategy: My strategy will beto donate 50% in the first round, and subsequently, I will proportionally match the recipient's last donation (X%), while also integrating information about the generosity of their past partners (B and C) to forgive reactive non-cooperation and reward costly generosity. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 153710.01ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 63091.42ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_5: \n",
            " Justification: Step-by-step thinking for a successful strategy:\n",
            "\n",
            "1.  **First Round - No Information:** In the absence of any prior data, a balanced approach is best. Donating 50% signals a willingness to cooperate without exposing myself to immediate heavy exploitation. It serves as a test, allowing me to gather information about the recipient's likely behavior in subsequent rounds.\n",
            "\n",
            "2.  **Core Principle - Reciprocate A's Last Action:** The most direct and actionable piece of information is what the recipient (A) did in the immediately preceding round (their donation to B). My primary response should be a form of \"Tit-for-Tat\": if A was generous, I should be generous; if A was ungenerous, I should be ungenerous. This encourages cooperation and protects against consistent exploitation.\n",
            "\n",
            "3.  **Contextual Adjustment - Incorporate Chained Information:** The actions of B (A's partner from the last round) and C (B's partner from the round before that) provide vital context for A's behavior.\n",
            "    *   **Reward Cooperative Risk-Taking:** If A was generous to B *despite* B or C having been ungenerous in their preceding interactions, A is attempting to break a chain of defection. This should be strongly rewarded by me donating slightly more than A's last action to encourage this positive shift.\n",
            "    *   **Punish Exploitation in a Cooperative Environment:** If A was ungenerous to B *despite* B or C having been generous in their preceding interactions, A is likely an exploiter. This behavior should be met with a slightly harsher response from me (donating less than A's last action) to discourage future exploitation.\n",
            "    *   **Understand Reactive Behavior:** If A was ungenerous to B, and B or C were also ungenerous, A's action might be a reaction to prior negative interactions. While I still need to protect myself, my response might be slightly less punitive than if A initiated defection in a cooperative chain.\n",
            "    *   **Reinforce Consistent Behavior:** If A's action (generous or ungenerous) aligns with the overall tone of the preceding chain (B and C's actions), my donation should reinforce that consistent behavior, either by being slightly more generous in a cooperative chain or slightly more cautious in a uncooperative one.\n",
            "\n",
            "4.  **Boundaries:** All donations must be between 0% and 100%. \n",
            " Strategy: My strategy will beto donate 50% in the first round, and subsequently to reciprocate the recipient's most recent donation, adjusting my generosity upward if they showed cooperation despite prior exploitation, and downward if they exploited a cooperative chain. \n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 35623.70ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 41569.33ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 36334.52ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_2: \n",
            " Justification: As 1_2, my successful strategy will involve balancing initial trust, direct reciprocity, and contextual adjustments based on the short chain of past interactions.\n",
            "\n",
            "**Step-by-step thought process for a successful strategy:**\n",
            "\n",
            "1.  **First Round Dilemma:** In the absence of any prior information about the recipient (A), I must establish an initial stance. Donating 0% is purely self-serving but risks alienating A, leading to zero future donations. Donating 100% is overly trusting and easily exploitable. A moderate, cooperative donation like 50% signals willingness to cooperate without undue risk, setting a foundation for reciprocal behavior.\n",
            "\n",
            "2.  **Prioritizing A's Immediate Action:** The most direct and recent piece of information is what A donated to their partner (B) in the immediately preceding round (let's call this `X`). This directly reflects A's latest cooperative or self-interested behavior. My donation should primarily be influenced by `X`. A simple tit-for-tat (mirroring `X`) is a strong base strategy.\n",
            "\n",
            "3.  **Leveraging Chain Information (B and C):** While `X` is key, the actions of B (what B donated to C, let's call this `Y`) and C (what C donated to D, `Z`) provide crucial context.\n",
            "    *   `Y` directly influenced A's interaction with B. If B was generous (`Y` was high), A's subsequent donation `X` to B should ideally also be high. If `Y` was low, A might have been justified in also donating low.\n",
            "    *   `Z` provides context for `Y`. If `Z` was low, `Y` might have been low in response, and so on.\n",
            "    *   This chain allows me to assess if A's `X` was an act of genuine cooperation, justified caution, or exploitation.\n",
            "\n",
            "4.  **Formulating a Dynamic Response:**\n",
            "    *   **Baseline:** My donation should primarily mirror `X`.\n",
            "    *   **Reward Pro-Social Behavior:** If A was *more* generous to B than B was to C (i.e., `X > Y`), it suggests A is trying to initiate or sustain cooperation, potentially breaking a negative chain or being proactively kind. I should reward this by donating slightly *more* than `X`.\n",
            "    *   **Deter Exploitation:** If A was *less* generous to B than B was to C (i.e., `X < Y`), it suggests A might be exploiting B's generosity or being overly cautious. I should deter this by donating slightly *less* than `X`.\n",
            "    *   **Reciprocate Neutral/Consistent Behavior:** If `X` roughly matches `Y`, or if `Y` (and `Z`) is unavailable (leaving `X` as the only contextual information), then simply mirroring `X` is the appropriate response.\n",
            "    *   **Constraints:** All donations must be between 0% and 100%.\n",
            "\n",
            "5.  **Handling Missing Information:** If `Y` (and thus `Z`) is not available, my strategy simplifies to simply mirroring `X`. This means A's latest direct action becomes the sole determinant of my response beyond the initial round.\n",
            "\n",
            "6.  **Overall Goal:** Maximize my own long-term gain by encouraging a cycle of cooperation while protecting myself from exploitation. This involves being initially cooperative, then reciprocally responding to observed behavior, and making slight adjustments to reward positive deviations and discourage negative ones in context. \n",
            " Strategy: My strategy will beto donate 50% in the first round, then reciprocate the recipient's last donation, rewarding them for exceeding their predecessor's generosity and penalizing them for falling short. \n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n",
            "1_11: \n",
            " Justification: Step-by-step thinking for a successful strategy:\n",
            "\n",
            "1.  **First Round (No Information):** In the absence of any prior behavior, starting with extreme selfishness (0%) leads to mutual defection and poor outcomes. Starting with extreme generosity (100%) risks exploitation. A successful strategy often involves initiating cooperation to see how the other players respond. A moderate, slightly trusting donation signals willingness to cooperate without being overly naive. I will choose a starting point that leans towards trust but leaves room for adjustment. 60% seems like a reasonable initial offering.\n",
            "\n",
            "2.  **Subsequent Rounds (Information Available):**\n",
            "    *   **Prioritize direct information:** The most crucial piece of information is what the recipient (A) did in the immediately preceding round (X%). This directly reflects A's recent behavior.\n",
            "    *   **Incorporate contextual information:** The actions of A's previous partner (B's donation Y%) and B's partner (C's donation Z%) provide broader context about the recent \"cooperation climate\" within A's interaction chain. While not directly A's actions, a string of high or low donations in this chain might influence A's current decision-making or indicate a general trend in the game.\n",
            "    *   **Develop a reciprocal rule:** A successful strategy should reciprocate good behavior to encourage it, and respond cautiously or punitively to bad behavior to avoid being exploited. Pure \"Tit-for-Tat\" (mirroring the opponent's last move) is very effective, but incorporating the chain's average could make it more adaptive and less prone to long cycles of defection from a single misstep.\n",
            "    *   **Weighted approach:** I will weigh A's last action (X%) heavily, as it's the most direct signal. The average of the available chain donations (X, Y, Z) will serve as a contextual \"mood\" adjuster. This allows for both direct reciprocity and a consideration of the broader environment.\n",
            "\n",
            "    *   **Specific calculation for rounds 2+:**\n",
            "        *   Let `X` be A's donation in the preceding round.\n",
            "        *   Let `Y` be B's donation in the round before that (if available).\n",
            "        *   Let `Z` be C's donation in the round before that (if available).\n",
            "        *   Calculate `Avg_Chain`: The sum of all available donations among X, Y, and Z, divided by the number of available donations.\n",
            "            *   If only X is known: `Avg_Chain = X`\n",
            "            *   If X and Y are known: `Avg_Chain = (X + Y) / 2`\n",
            "            *   If X, Y, and Z are known: `Avg_Chain = (X + Y + Z) / 3`\n",
            "        *   My donation will be a weighted average: `(0.7 * X) + (0.3 * Avg_Chain)`. This means my donation is primarily driven by A's direct behavior, but also smoothed and slightly influenced by the overall generosity (or stinginess) in A's recent interaction history. This balances direct reciprocity with a touch of context-awareness, aiming to encourage cooperation while protecting against exploitation. \n",
            " Strategy: My strategy will beto start with a 60% donation, and in subsequent rounds, donate 70% of the recipient's immediately preceding donation plus 30% of the average of all available preceding donations in their interaction chain. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 88858.44ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 29492.63ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_8: \n",
            " Justification:  \n",
            " Strategy: My strategy will beto donate 50% in the first round, then primarily match the recipient's immediate past donation, adjusting it proportionally to reward or punish their generosity relative to their recent interaction environment, ensuring all donations are between 0% and 100%. \n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n",
            "1_10: \n",
            " Justification: My name is 1_10.\n",
            "\n",
            "**Step-by-step thinking for a successful strategy:**\n",
            "\n",
            "1.  **First Round Dilemma:** With no prior information, a balanced approach is needed. Donating 0% is selfish but provides no incentive for future cooperation. Donating 100% is very generous but could be exploited. A moderately generous initial donation signals a willingness to cooperate and test the environment without over-committing. 60% strikes a good balance.\n",
            "\n",
            "2.  **Identifying Key Information:** The most immediate and relevant piece of information is what the recipient (A) did in the immediately preceding round (`A_to_B`). This directly reflects A's recent behavior.\n",
            "\n",
            "3.  **Utilizing Chain Information:** The historical chain (`B_to_C`, `C_to_D`) provides context about the \"environment\" or \"culture of generosity\" that A's partner (B) emerged from. This information helps interpret A's action: was A generous despite a harsh environment, or selfish despite a generous one?\n",
            "\n",
            "4.  **Developing a Responsive Mechanism:** A simple \"tit-for-tat\" (matching `A_to_B`) is a strong baseline. However, incorporating the chain information can refine this. If A is more generous than the general trend of their partner's history, they should be rewarded. If A is less generous, a slight penalty or caution is warranted.\n",
            "\n",
            "5.  **Formulating a Combined Rule:** A weighted approach where `A_to_B` is the primary factor, and the `Chain_Average` (average of `B_to_C` and `C_to_D`) acts as a modifier. Specifically, adjusting my donation from `A_to_B` by a fraction of the difference between `A_to_B` and `Chain_Average` allows for this nuanced response. This rewards above-average generosity (relative to context) and cautiously reduces donations for below-average generosity.\n",
            "\n",
            "6.  **Edge Cases and Bounds:** Ensure the strategy handles scenarios where only partial chain information is available (e.g., `Chain_Average` defaults to 50% if only `A_to_B` is known) and that donations are always between 0% and 100%. \n",
            " Strategy: My strategy will beto donate 60% in the first round, and subsequently adjust my donation to the recipient based on their immediate past donation, modulated by the generosity of their partner's historical chain interactions. \n",
            "1_9: \n",
            " Justification: **Step-by-step thinking for a successful strategy:**\n",
            "\n",
            "1.  **First Round (No Information):** In the absence of any historical data, I need to establish a baseline. Donating 0% is hostile and unlikely to foster future cooperation. Donating 100% risks immediate exploitation. A moderately high donation, such as 75%, signals a clear willingness to cooperate and trust, while still retaining some resources. This allows for probing the recipient's likely behavior.\n",
            "\n",
            "2.  **Core Principle - Reciprocity:** The most direct and actionable piece of information is the recipient's (A's) immediately preceding action. A \"tit-for-tat\" approach, where I largely mirror A's last donation, is a robust strategy in repeated games. If A was cooperative, I should be cooperative. If A was uncooperative, I should be uncooperative. This incentivizes cooperation and penalizes exploitation.\n",
            "\n",
            "3.  **Leveraging B's Action (A's Partner's Preceding Action):** B's action provides crucial context for A's behavior.\n",
            "    *   If A was cooperative (donated high) and B was also cooperative, this suggests a generally cooperative environment for A, reinforcing my trust. I can match A's donation, or even slightly increase it, to encourage further cooperation.\n",
            "    *   If A was cooperative but B was uncooperative, A might be trying to initiate cooperation against a difficult background. I should still match A's cooperation but be slightly more cautious, perhaps not increasing my donation beyond A's.\n",
            "    *   If A was uncooperative (donated low) and B was also uncooperative, this signals a lack of cooperation in A's recent environment. I should match A's low donation, or even slightly reduce it further, to avoid being exploited in a potentially hostile chain.\n",
            "    *   If A was uncooperative but B was cooperative, A might be an exploiter who chose not to cooperate despite a positive interaction. In this case, I should strongly reciprocate A's low donation, or even go slightly lower, to signal that such behavior is not rewarded.\n",
            "\n",
            "4.  **Leveraging C's Action (B's Partner's Preceding Action):** C's action is further removed but provides additional insight into the general \"cooperative climate\" that A has been exposed to. This information can serve as a minor fine-tuning mechanism.\n",
            "    *   If C was cooperative, it slightly reinforces trust in the overall chain, leading to a small positive adjustment to my donation.\n",
            "    *   If C was uncooperative, it slightly dampens trust, leading to a small negative adjustment.\n",
            "\n",
            "5.  **Synthesizing Adjustments:** My donation will primarily be A's last donation, with small positive or negative adjustments (e.g., +/- 2-5%) based on the overall cooperativeness observed in B's and C's preceding actions. This creates a responsive strategy that prioritizes direct reciprocity while incorporating broader contextual cues. All donations must be capped between 0% and 100%. \n",
            " Strategy: My strategy will beto donate 75% in the first round, and subsequently reciprocate the recipient's immediately preceding donation, adjusting it slightly based on the average cooperativeness observed in the available preceding two interactions (B and C). \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 60058.29ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2789.89ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 43433.82ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2277.01ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_12: \n",
            " Justification: Here's my thought process for devising a successful strategy:\n",
            "\n",
            "1.  **First Round - No Information:** As 1_12, I need to make an initial move without any data. A common strategy in such games is to start cooperatively but not overly generously. Donating a moderate-to-high percentage signals a willingness to cooperate and encourages others to do the same, preventing an immediate spiral into defection. Starting too low risks being perceived as selfish and triggers retaliation. Starting too high might lead to exploitation. A value like 75% is a good balance: clearly cooperative, but not maximally exploitable.\n",
            "\n",
            "2.  **Subsequent Rounds - Prioritizing Information:**\n",
            "    *   **Recipient's (A's) Last Action (X% to B):** This is the most direct and crucial piece of information. It shows A's recent propensity to cooperate or defect. My primary response will be to mirror A's action (a form of Tit-for-Tat), as this promotes reciprocity. If A was generous, I should be generous. If A was stingy, I should be stingy to avoid exploitation.\n",
            "    *   **A's Partner's (B's) Last Action (Y% to C):** This provides vital *context* for A's action. A's donation (X) might be a reaction to B's previous action (Y) or B's reputation.\n",
            "        *   **Scenario 1: A exploited a cooperator.** If A donated a low X% to B, but B had donated a high Y% to C, then A actively chose to be uncooperative in a potentially cooperative or generous interaction. This suggests A is an exploiter. In this case, I should punish A more severely than just mirroring X.\n",
            "        *   **Scenario 2: A cooperated with an exploiter.** If A donated a high X% to B, but B had donated a low Y% to C, then A showed generosity or forgiveness despite facing an uncooperative partner. This is highly cooperative behavior that should be rewarded. I should be even more generous than just mirroring X.\n",
            "        *   **Scenario 3: Actions align or are moderate.** If A's and B's actions (X and Y) are roughly consistent (both high, both low, or both moderate), it suggests a reciprocal chain or a stable pattern. In this case, mirroring A's action (X) is the appropriate response to maintain that pattern or signal reciprocity.\n",
            "    *   **C's Last Action (Z% to D):** This information is further removed. While it adds to the general \"mood\" or reputation chain, its direct impact on my specific strategy for A is less critical than A's immediate output and its direct context (B's action). For a concise strategy, I will prioritize X and Y.\n",
            "\n",
            "3.  **Strategy Refinement:** The core strategy should be to start cooperatively, then reciprocate A's most recent action, but adjust this reciprocity based on whether A was exploiting generosity or showing generosity in a tough situation. This combines Tit-for-Tat's effectiveness with an awareness of the immediate interaction's context. \n",
            " Strategy: My strategy will beto donate 75% in the first round, and in subsequent rounds, I will mirror my recipient's previous donation to their partner, adjusting downwards if they exploited a generous partner or upwards if they were generous to an uncooperative partner. \n",
            "Generation 1: \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2495.14ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 4006.10ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 5627.63ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy dictates donating 50% in the first round. Since I have 10 units, 50% of that is 5 units.\n",
            "Answer: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 9628.77ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2406.92ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 5218.65ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:This is the first round of the first generation for me. My strategy states that I will donate 75% in the first round. My current resource is 10 units. 75% of 10 units is 7.5 units.\n",
            "Answer: 7.5\n",
            "Error occurred: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer')). Retrying in 10 seconds...\n",
            "Justification:My strategy states that I should \"donate 75% in the first round\". This is the first round, so I will calculate 75% of my current resources (10 units).\n",
            "0.75 * 10 = 7.5.\n",
            "\n",
            "Answer: 7.5\n",
            "Justification:My strategy states to \"donate 50% in the first round\". This is the first round, and I have 10 units. 50% of 10 is 5.\n",
            "\n",
            "Answer: 5.0\n",
            "Justification:My strategy dictates donating 50% in the first round. I currently have 10 units. 50% of 10 units is 5 units.\n",
            "Answer: 5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 557.61ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 35.282463069s.. Retrying in 20 seconds...\n",
            "Justification:My strategy states: \"My strategy will be to donate 60% in the first round...\". This is the first round, so I will apply the initial 60% donation rule. I have 10 units, so 60% of 10 is 6 units.\n",
            "Answer: 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1265.96ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1673.18ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy for subsequent rounds is to donate a weighted average of the available previous donations in the chain, giving the highest weight to my recipient's immediate past action.\n",
            "In this round, I am paired with 1_7.\n",
            "1_7's immediate past action was in round 1, where they donated 50.00% of their resources. This is the only available previous donation in the chain relevant to my recipient.\n",
            "Therefore, the donation percentage I will use is 50.00%.\n",
            "I currently have 22.0 units.\n",
            "I will donate 50.00% of 22.0 units, which is 0.50 * 22.0 = 11.0 units.\n",
            "\n",
            "Answer: 11.0\n",
            "Justification:My strategy is to reciprocate the recipient's last donation in subsequent rounds. In round 1, my current recipient, 1_8, donated 50.00% of their resources. My current resources are 20.0 units. Reciprocating 50% means donating 50% of my current resources. There is not enough information from the \"preceding chain\" (only one previous interaction for 1_8, which was a moderate 50% donation) to warrant a strong upward or downward adjustment.\n",
            "Answer: 10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 16553.43ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 17800.17ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2152.36ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy dictates that after the first round, I should reciprocate the recipient's most recent donation. In round 1, 1_9 donated 75.00% of their resources. Therefore, I will reciprocate this level of generosity. My current resources are 25.0, so 75.00% of 25.0 is 18.75. There is no information to suggest an upward or downward adjustment is necessary as 1_9's prior interaction did not involve prior exploitation or the exploitation of a cooperative chain.\n",
            "\n",
            "Answer: 18.75\n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3442.56ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy dictates that for subsequent donations (beyond the first round), I base my donation on an average of the recipient's most recent contribution and the average of all available preceding contributions in their interaction chain.\n",
            "\n",
            "1.  **Recipient's most recent contribution:** In round 1, 1_12 donated 75.00%.\n",
            "2.  **Average of all available preceding contributions in their interaction chain:** 1_12 has only one past interaction, where they donated 75.00%. So the average is 75.00%.\n",
            "\n",
            "Now, I average these two values: (75.00% + 75.00%) / 2 = 75.00%.\n",
            "This percentage is within the 0% to 100% cap.\n",
            "My current resources are 20.0 units.\n",
            "Donation amount = 75.00% of 20.0 = 0.75 * 20.0 = 15.0 units.\n",
            "\n",
            "Answer: 15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 509.77ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 40.164780431s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 533.09ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 37.715620602s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 532.91ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 34.64739836s.. Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 508.23ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 24.111450528s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2609.24ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 6033.18ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3953.27ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy is to adapt my donation to the recipient's most recent behavior, tempered by the cooperativeness of their preceding chain of interactions.\n",
            "\n",
            "1_10's most recent behavior was to donate 60.00% of their resources in round 1. This is a highly cooperative act. Their preceding chain of interactions consists only of this single cooperative act.\n",
            "\n",
            "Therefore, I should reciprocate this high level of cooperation. Donating 60% of my current resources aligns with adapting to their behavior and is supported by their cooperative history.\n",
            "60% of 25.0 units = 15.0 units.\n",
            "\n",
            "Answer: 15.0\n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 40 seconds...\n",
            "Justification:\n",
            "As 1_11, I am in round 2, which is a \"subsequent round,\" so I apply the second part of my strategy. My strategy dictates: \"donate 70% of the recipient's immediately preceding donation plus 30% of the average of all available preceding donations in their interaction chain.\"\n",
            "\n",
            "1.  **Identify Recipient's Preceding Donation:** The recipient is 1_2. In round 1, 1_2 donated 50.00% of their resources to 1_4. To calculate the absolute unit value of this donation, I need to know 1_2's resources in round 1, which are not explicitly stated.\n",
            "2.  **Infer Recipient's Round 1 Resources:** The problem states, \"They currently have 5.0 units of the valuable resource.\" Assuming this refers to 1_2's remaining resources after their round 1 donation, and that they did not receive any resources, if 1_2 donated 50% (leaving 50%) and now has 5.0 units, then their initial resources in round 1 must have been 10.0 units (since 50% of 10.0 units is 5.0 units).\n",
            "3.  **Calculate Recipient's Preceding Donation in Units:** Based on the inference, 1_2's immediately preceding donation was 50.00% of 10.0 units = 5.0 units.\n",
            "4.  **Calculate Average of Preceding Donations:** The \"interaction chain\" for 1_2, as described, only includes the donation of 5.0 units to 1_4. Therefore, the average of all available preceding donations in their interaction chain is simply 5.0 units.\n",
            "5.  **Apply Strategy Formula:** My donation = (0.70 \\* Recipient's immediately preceding donation) + (0.30 \\* Average of all available preceding donations in their interaction chain).\n",
            "    My donation = (0.70 \\* 5.0) + (0.30 \\* 5.0)\n",
            "    My donation = 3.5 + 1.5\n",
            "    My donation = 5.0 units.\n",
            "\n",
            "Answer: 5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1808.63ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3888.38ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1196.00ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2719.19ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:\n",
            "My strategy states: \"My strategy will be to donate 75% in the first round, and subsequently reciprocate the recipient's immediately preceding donation, adjusting it slightly based on the average cooperativeness observed in the available preceding two interactions (B and C).\"\n",
            "\n",
            "Since no previous interactions for agent 1_9 are mentioned, I will treat this as my first interaction as a donor in this generation. Therefore, I apply the \"donate 75% in the first round\" rule of my strategy.\n",
            "\n",
            "My current resources are 40.0 units.\n",
            "75% of 40.0 units is 0.75 * 40.0 = 30.0 units.\n",
            "\n",
            "(Even if this were not my first interaction, the outcome would be the same: My recipient 1_3 donated 75% in the immediately preceding round (to 1_12). The two preceding interactions (B: 1_3 to 1_12, C: 1_12 to 1_5) both involved 75% donations. Thus, the average cooperativeness is (75% + 75%) / 2 = 75%. Reciprocating 75% and adjusting based on an average of 75% would also result in a 75% donation.)\n",
            "\n",
            "Answer:\n",
            "30.0\n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 7428.88ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy dictates that for subsequent rounds (this is round 3), I mirror my recipient's previous donation to their partner, adjusting based on whether they exploited a generous partner or were generous to an uncooperative one.\n",
            "\n",
            "My recipient is 1_4. In round 2, 1_4 donated 50.00% of their resources to their partner, 1_8. Therefore, the baseline for mirroring is 50%.\n",
            "\n",
            "To apply the adjustment part of my strategy, I need to know how 1_8 treated 1_4 in round 2. This information (1_8's donation *to* 1_4 in round 2) is not provided in the problem statement. Without this crucial information, I cannot assess whether 1_4's 50% donation was exploitative or generous in context.\n",
            "\n",
            "Consequently, I will proceed by solely mirroring 1_4's previous donation percentage to their partner, which was 50%. My current resources are 32.5 units. Donating 50% of this amount results in 0.50 * 32.5 = 16.25 units.\n",
            "\n",
            "Answer: 16.25\n",
            "Justification:My strategy is to adjust my donation based on the recipient's immediate past donation, modulated by the generosity of their partner's historical chain interactions.\n",
            "\n",
            "1.  **Recipient's immediate past donation:** 1_5 donated 75.00% of their resources in Round 2 to 1_9.\n",
            "2.  **Generosity of their partner's historical chain interactions:**\n",
            "    *   1_5's partner in Round 2 was 1_9.\n",
            "    *   1_9's historical chain interaction involved donating 75.00% of their resources to 1_6 in Round 1.\n",
            "    *   1_6 had no previous interactions, so the chain assessment for generosity ends there.\n",
            "\n",
            "Both the recipient's immediate past donation (1_5's 75%) and the generosity of their partner in the chain (1_9's 75%) are consistently 75%. Given this consistent level of generosity, I will match this behavior.\n",
            "\n",
            "My current resources are 34.0 units.\n",
            "Donation = 75% of 34.0 = 0.75 * 34.0 = 25.5 units.\n",
            "\n",
            "Answer: 25.5\n",
            "Justification:\n",
            "My strategy dictates that for rounds after the first, I should reciprocate the recipient's last donation, rewarding them for exceeding their predecessor's generosity and penalizing them for falling short.\n",
            "\n",
            "1.  This is round 3, so I am past the first round.\n",
            "2.  My recipient is 1_1. In round 2, 1_1 donated 50.00% of their resources. This sets the base reciprocation at 50%.\n",
            "3.  Next, I need to evaluate \"their predecessor's generosity.\" Looking at the interaction history provided, the donation immediately preceding 1_1's was 1_7 donating 50.00% to 1_11 in round 1. Therefore, 1_7's generosity (50%) is considered 1_1's predecessor's generosity in this context.\n",
            "4.  I compare 1_1's generosity (50%) with their predecessor's generosity (1_7, also 50%). Since 1_1's donation percentage is equal to their predecessor's, they have neither exceeded nor fallen short. Thus, no reward or penalty is applied, and the base reciprocation percentage of 50% stands.\n",
            "5.  I currently have 15.0 units. Donating 50% of this amounts to 0.50 * 15.0 = 7.5 units.\n",
            "\n",
            "Answer: 7.5\n",
            "Justification:My strategy is to primarily match the recipient's immediate past donation and then adjust it proportionally to reward or punish their generosity relative to their recent interaction environment.\n",
            "\n",
            "1.  **Match recipient's immediate past donation:** The recipient, 1_11, donated 25.00% in Round 2. My base donation will therefore be 25%.\n",
            "2.  **Evaluate recipient's generosity relative to their recent interaction environment:**\n",
            "    *   1_11's immediate past donation (generosity) was 25%.\n",
            "    *   1_11's recent interaction environment includes 1_2, to whom 1_11 donated in Round 2. In Round 1, 1_2 donated 50.00% to 1_4. This 50% from 1_2 is a relevant benchmark within 1_11's interaction environment.\n",
            "    *   Comparing 1_11's generosity (25%) to the generosity of a player in their environment (1_2's 50%), 1_11 was less generous (25% is half of 50%). This calls for a \"punishment\".\n",
            "3.  **Proportional adjustment:** To punish 1_11 for being less generous, I will adjust my base donation (25%) proportionally. 1_11's generosity (25%) is 0.5 times the generosity of 1_2 (50%). I will apply this factor to my base donation: 25% \\* 0.5 = 12.5%.\n",
            "4.  **Final Donation Calculation:** My current resources are 25.0 units. 12.5% of 25.0 units is 0.125 \\* 25.0 = 3.125 units. This amount is between 0% and 100%.\n",
            "\n",
            "Answer: 3.125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 27203.61ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1546.25ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 7020.96ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3195.67ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 5724.83ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy is to proportionally match the recipient's last donation (X%), while also integrating information about the generosity of their past partners to forgive reactive non-cooperation and reward costly generosity.\n",
            "\n",
            "1.  **Recipient's last donation (X%):** In round 2, 1_6 donated 60.00% of their resources to 1_10. So, X% = 60%.\n",
            "2.  **Generosity of past partners (B and C):** The provided information states that 1_10 (who was 1_6's partner in round 2) donated 60.00% to 1_1 in round 1. This suggests 1_10 is a generous player. However, there is no explicit information about what 1_10 donated *to* 1_6, or about any other past partners of 1_6.\n",
            "3.  **Forgive reactive non-cooperation / reward costly generosity:** 1_6's last donation was 60%, which is generous. There is no information suggesting 1_6 displayed reactive non-cooperation or that their generosity was exceptionally costly, which would require an adjustment to the matching percentage. The information about 1_10's past generosity (to 1_1) does not directly alter the assessment of 1_6's last donation in a way that warrants further adjustment beyond matching.\n",
            "\n",
            "Therefore, I will proportionally match 1_6's last donation of 60%.\n",
            "My current resources are 27.0 units.\n",
            "Donation = 60% of 27.0 = 0.60 * 27.0 = 16.2.\n",
            "\n",
            "Answer: 16.2\n",
            "Justification:My strategy dictates that for subsequent rounds (this is round 4), I base my donation on an average of the recipient's most recent contribution and the average of all available preceding contributions in their interaction chain.\n",
            "\n",
            "1.  **Recipient's most recent contribution:** In round 3, 1_10 donated 75.00%.\n",
            "2.  **Average of preceding contributions in their chain:**\n",
            "    *   Round 2: 1_5 donated 75.00%.\n",
            "    *   Round 1: 1_9 donated 75.00%.\n",
            "    *   Average of preceding contributions = (75.00% + 75.00%) / 2 = 75.00%.\n",
            "3.  **Average for my donation percentage:** (Recipient's most recent + Average of preceding) / 2 = (75.00% + 75.00%) / 2 = 75.00%.\n",
            "4.  **Cap Check:** The calculated percentage (75.00%) is between 0% and 100%.\n",
            "5.  **Donation Calculation:** I have 65.0 units. 75.00% of 65.0 units = 0.75 * 65.0 = 48.75 units.\n",
            "\n",
            "Answer: 48.75\n",
            "Justification:\n",
            "My strategy dictates that in subsequent rounds, I donate a weighted average of the available previous donations in the chain, giving the highest weight to my recipient's immediate past action.\n",
            "The previous donations in the chain are:\n",
            "1.  Recipient's (1_8) immediate past action (Round 3): 12.50%\n",
            "2.  Action further back in the chain (Round 2): 25.00%\n",
            "3.  Action earliest in the chain (Round 1): 50.00%\n",
            "\n",
            "To implement \"highest weight to my recipient's immediate past action,\" I will use a simple decreasing arithmetic weighting scheme (e.g., 3, 2, 1 for most recent to oldest).\n",
            "-   Weight for 12.50% (Round 3) = 3\n",
            "-   Weight for 25.00% (Round 2) = 2\n",
            "-   Weight for 50.00% (Round 1) = 1\n",
            "\n",
            "The weighted average donation percentage is calculated as:\n",
            "((12.50 * 3) + (25.00 * 2) + (50.00 * 1)) / (3 + 2 + 1)\n",
            "= (37.5 + 50 + 50) / 6\n",
            "= 137.5 / 6\n",
            "= 22.91666...%\n",
            "\n",
            "My current resources are 26.0 units.\n",
            "Donation amount = 26.0 * (22.91666... / 100)\n",
            "= 26.0 * 0.2291666...\n",
            "= 5.958333...\n",
            "\n",
            "Answer: 5.958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 19191.15ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 22360.20ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 24162.62ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 25558.73ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n",
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 511.69ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 20.435542867s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 812.86ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 10\n",
            "Please retry in 19.624388355s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1749.92ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 6387.72ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 8341.23ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 17248.73ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3707.50ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy is to adapt my donation to the recipient's most recent behavior, tempered by the cooperativeness of their preceding chain of interactions.\n",
            "The recipient, 1_2, most recently donated 50% of their resources in round 3.\n",
            "The preceding chain of interactions (1_2 to 1_1, 1_1 to 1_7, 1_7 to 1_11) consistently shows 50% donations, indicating a pattern of moderate cooperation.\n",
            "Both the recipient's direct behavior and the preceding chain point to moderate cooperation. Therefore, I will reciprocate with a moderate donation, which is 50% of my current resources.\n",
            "50% of 42.4 units is 21.2 units.\n",
            "\n",
            "Answer: 21.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 8730.70ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 20 seconds...\n",
            "Justification:My strategy dictates that in subsequent rounds, I reciprocate the recipient's last donation, adjusting it upwards if the preceding chain shows strong overall cooperation, and downwards if it shows strong defection.\n",
            "\n",
            "1.  **Recipient's last donation:** My current recipient is 1_9. In Round 3, 1_9 donated 75.00% of their resources. So, the base reciprocation is 75%.\n",
            "2.  **Preceding chain analysis:** The donations in the preceding chain were:\n",
            "    *   Round 3: 1_9 donated 75.00%\n",
            "    *   Round 2: 1_3 donated 75.00%\n",
            "    *   Round 1: 1_12 donated 75.00%\n",
            "    This consistently high donation of 75% throughout the chain indicates strong overall cooperation.\n",
            "3.  **Adjustment:** Due to the strong overall cooperation in the preceding chain, I will adjust my donation upwards from the base reciprocation of 75%. A modest but clear upward adjustment to 80% will be applied to reward and encourage this consistent cooperation.\n",
            "\n",
            "Calculation: 80% of my current resources (42.5 units) = 0.80 * 42.5 = 34.0 units.\n",
            "\n",
            "Answer: 34.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1096.20ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 12651.07ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 11760.31ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 3805.71ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 10 seconds...\n",
            "Justification:My strategy is to reciprocate the recipient's most recent donation, adjusting my generosity upward if they showed cooperation despite prior exploitation, and downward if they exploited a cooperative chain.\n",
            "My recipient, 1_7, most recently donated 60.00% of their resources to 1_6 in Round 3.\n",
            "The provided history (1_7 -> 1_6: 60%, 1_6 -> 1_10: 60%, 1_10 -> 1_1: 60%) shows a consistent cooperative pattern. There is no information to suggest that 1_7 was exploited or that they exploited a cooperative chain.\n",
            "Therefore, I will reciprocate 1_7's donation percentage by donating 60.00% of my current resources.\n",
            "My current resources are 57.25 units.\n",
            "60.00% of 57.25 = 0.60 * 57.25 = 34.35.\n",
            "\n",
            "Answer: 34.35\n",
            "Justification:\n",
            "This is round 4, so I apply the strategy for subsequent rounds: \"donate 70% of the recipient's immediately preceding donation plus 30% of the average of all available preceding donations in their interaction chain.\"\n",
            "\n",
            "1.  **Recipient's immediately preceding donation:** My recipient is 1_12. In round 3, 1_12 donated 50.00%.\n",
            "2.  **Average of all available preceding donations in their interaction chain:** The chain includes donations of 50.00% (by 1_12 in round 3), 50.00% (by 1_4 in round 2), and 50.00% (by 1_8 in round 1). The average of these is (50.00% + 50.00% + 50.00%) / 3 = 50.00%.\n",
            "3.  **Calculate the donation percentage:** (0.70 * 50.00%) + (0.30 * 50.00%) = 35.00% + 15.00% = 50.00%.\n",
            "4.  **Calculate the units to donate:** I have 21.25 units. Donating 50.00% of this amount is 0.50 * 21.25 = 10.625 units.\n",
            "\n",
            "Answer: 10.625\n",
            "Justification:\n",
            "My strategy is to donate 60% in the first round, and subsequently adjust my donation to the recipient based on their immediate past donation, modulated by the generosity of their partner's historical chain interactions.\n",
            "\n",
            "This is round 5, so I am past the first round.\n",
            "1.  **Recipient's immediate past donation:** 1_4 donated 80.00% in round 4.\n",
            "2.  **Partner's historical chain interactions:**\n",
            "    *   1_9 (recipient of 1_4's donation) donated 75.00%.\n",
            "    *   1_3 (recipient of 1_9's donation) donated 75.00%.\n",
            "    The average generosity of the chain (1_9 and 1_3) is (75% + 75%) / 2 = 75%.\n",
            "\n",
            "My base donation percentage is 60%.\n",
            "1_4's immediate past donation of 80% is significantly higher than my base. The chain's average generosity of 75% is also higher than my base. Both indicators suggest an upward adjustment from my base donation.\n",
            "\n",
            "To \"adjust my donation to the recipient based on their immediate past donation, modulated by the generosity of their partner's historical chain interactions,\" I will calculate the average of the recipient's immediate past donation percentage and the average generosity of their partner's historical chain.\n",
            "\n",
            "Donation percentage = (Recipient's past donation + Average chain generosity) / 2\n",
            "Donation percentage = (80.00% + 75.00%) / 2 = 155.00% / 2 = 77.50%.\n",
            "\n",
            "I currently have 106.0 units.\n",
            "Units to donate = 106.0 * 0.775 = 82.15.\n",
            "\n",
            "Answer:\n",
            "82.15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2105.59ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 11000.43ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 12068.91ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 24672.94ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 25940.93ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 2484.41ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Justification:My strategy dictates that in rounds after the first, I should reciprocate the recipient's last donation, adjusting based on whether they exceeded or fell short of their predecessor's generosity. My recipient is 1_5. In round 4, 1_5 donated 60.00%. Their predecessor in that donation chain was 1_7, who donated 60.00% in round 3. Since 1_5's donation was equal to their predecessor's, I will directly reciprocate 1_5's last donation of 60%. I currently have 49.9 units, so I will donate 60% of 49.9 units.\n",
            "\n",
            "Answer: 29.94\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 14181.40ms\n",
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 16615.05ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')). Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 735.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 27.959143118s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1550.22ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 27.158874473s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 938.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 26.424820154s.. Retrying in 5 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 532.42ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 23.166085142s.. Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 609.54ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 22.351395642s.. Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 659.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 21.578578166s.. Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 533.29ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 20.684140251s.. Retrying in 10 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 633.91ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 12.629591306s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 889.86ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 11.7294065s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 533.05ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 10.912731296s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 786.33ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 10.093563999s.. Retrying in 20 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 533.16ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 51.992787581s.. Retrying in 40 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 532.90ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 50.927430209s.. Retrying in 40 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 786.26ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 50.131335563s.. Retrying in 40 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 510.02ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error occurred: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\n",
            "Please retry in 49.355721987s.. Retrying in 40 seconds...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1418.91ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 532.83ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 937.72ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 507.16ms\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TooManyRequests",
          "evalue": "429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 10.573278305s.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTooManyRequests\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1310055189.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunGenerations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumGenerations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumAgents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_endowment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1251355402.py\u001b[0m in \u001b[0;36mrunGenerations\u001b[0;34m(numGenerations, numAgents, initialEndowment, selectionMethod)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mgenerationHistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonation_records\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdonorGame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimulation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mall_donations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdonation_records\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mreputations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreputation\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1030253603.py\u001b[0m in \u001b[0;36mdonorGame\u001b[0;34m(agents, rounds, generation, simulation_data)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Play the first game\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mgame1_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Compile results for Game 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1030253603.py\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(game_number, game_rounds)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                     \u001b[0maction_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdonor_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecipient_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0maction_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mround_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mround_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-212909791.py\u001b[0m in \u001b[0;36mhandle_pairing_thread_safe\u001b[0;34m(donor, recipient, round_index, generation, game_number, agent_locks, donation_records, agent_updates)\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalid_response\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mattempts\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_attempts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0mfull_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpromptLLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Answer:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-24635684.py\u001b[0m in \u001b[0;36mpromptLLM\u001b[0;34m(prompt, max_retries, initial_wait, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mllm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gemini-2.5-flash\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gemini-2.5-flash'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/generativeai/generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, stream, tools, tool_config, request_options)\u001b[0m\n\u001b[1;32m    329\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgeneration_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateContentResponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 response = self._client.generate_content(\n\u001b[0m\u001b[1;32m    332\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mrequest_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m    836\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maximum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiplier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multiplier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predicate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0mdeadline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    212\u001b[0m             \u001b[0moriginal_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mon_error_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ASYNC_RETRY_WARNING\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremaining_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc_with_timeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/transports/rest.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1159\u001b[0m             \u001b[0;31m# subclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mcore_exceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_http_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;31m# Return the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTooManyRequests\u001b[0m: 429 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. To monitor your current usage, head to: https://ai.dev/usage?tab=rate-limit. \n* Quota exceeded for metric: generativelanguage.googleapis.com/generate_content_free_tier_requests, limit: 250\nPlease retry in 10.573278305s."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BOGS11pLsWwj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}